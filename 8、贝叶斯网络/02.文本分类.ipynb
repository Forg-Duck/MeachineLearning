{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "题目：（源文件下载不了，无法运行）\n",
    "\n",
    "2万个原始文本资料分为20个类别，根据时间的先后顺序划分为训练集和测试集，进行文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer   # TF-idf 文本向量化包\n",
    "from sklearn.linear_model import RidgeClassifier              # Ridge回归\n",
    "from sklearn.neighbors import KNeighborsClassifier            # K临近回归\n",
    "from sklearn.svm import SVC                                   # SVM\n",
    "from sklearn.ensemble import RandomForestClassifier           # 随机森林\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.准备好各种学习模型及参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义包含了各种分类器和计算用时的函数\n",
    "def test_clf(clf):\n",
    "    print(u'分类器：', clf)\n",
    "    alpha_can = np.logspace(-3, 2, 10)\n",
    "    model = GridSearchCV(clf, param_grid={'alpha': alpha_can}, cv=5)\n",
    "    m = alpha_can.size\n",
    "    \n",
    "    # Ridge回归 及 参数alpha的选择\n",
    "    if hasattr(clf, 'alpha'):\n",
    "        model.set_params(param_grid={'alpha': alpha_can})\n",
    "        m = alpha_can.size\n",
    "        \n",
    "    # K临近 及 参数K（领域）的选择\n",
    "    if hasattr(clf, 'n_neighbors'):\n",
    "        neighbors_can = np.arange(1, 15)  # 这里取（1,15）\n",
    "        model.set_params(param_grid={'n_neighbors': neighbors_can})\n",
    "        m = neighbors_can.size\n",
    "        \n",
    "    # SVM 及 参数 C和gamma的选择\n",
    "    if hasattr(clf, 'C'):\n",
    "        C_can = np.logspace(1, 3, 3)\n",
    "        gamma_can = np.logspace(-3, 0, 3)\n",
    "        model.set_params(param_grid={'C':C_can, 'gamma':gamma_can})\n",
    "        m = C_can.size * gamma_can.size\n",
    "    \n",
    "    # 随机森林 及 参数最大深度的选择\n",
    "    if hasattr(clf, 'max_depth'):\n",
    "        max_depth_can = np.arange(4, 10)\n",
    "        model.set_params(param_grid={'max_depth': max_depth_can})\n",
    "        m = max_depth_can.size\n",
    "    \n",
    "    t_start = time()\n",
    "    model.fit(x_train, y_train)  # 训练模型； 并在开头和结尾卡了时间，计算用了多久\n",
    "    t_end = time()\n",
    "    t_train = (t_end - t_start) / (5*m)  # 计算出训练一个样本所用的时间\n",
    "    print(u'5折交叉验证的训练时间为：%.3f秒/(5*%d)=%.3f秒' % ((t_end - t_start), m, t_train))\n",
    "    print(u'最优超参数为：', model.best_params_)\n",
    "    \n",
    "    t_start = time()\n",
    "    y_hat = model.predict(x_test) # 给出预测； 并在开头和结尾卡了时间，计算用了多久\n",
    "    t_end = time()\n",
    "    t_test = t_end - t_start   # 计算预测用时\n",
    "    print(u'测试时间：%.3f秒' % t_test)\n",
    "    \n",
    "    acc = metrics.accuracy_score(y_test, y_hat)  # 计算模型的得分\n",
    "    print(u'测试集准确率：%.2f%%' % (100 * acc))\n",
    "    name = str(clf).split('(')[0]\n",
    "    \n",
    "    # 为了好看，没什么用\n",
    "    index = name.find('Classifier')\n",
    "    if index != -1:\n",
    "        name = name[:index]     # 去掉末尾的Classifier\n",
    "    if name == 'SVC':\n",
    "        name = 'SVM'\n",
    "    return t_train, t_test, 1-acc, name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.对数据进行预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载数据并分为训练和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始下载/加载数据...\n"
     ]
    }
   ],
   "source": [
    "# 源文件下载不了\n",
    "print(u'开始下载/加载数据...')\n",
    "\n",
    "t_start = time()\n",
    "\n",
    "# remove = ('headers', 'footers', 'quotes')\n",
    "remove = ()\n",
    "categories = 'alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space'\n",
    "# categories = None     # 若分类所有类别，请注意内存是否够用\n",
    "\n",
    "# 将数据分为测试数据和训练数据\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=0, remove=remove)\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=0, remove=remove)\n",
    "\n",
    "t_end = time()\n",
    "\n",
    "print(u'下载/加载数据完成，耗时%.3f秒' % (t_end - t_start))\n",
    "print(u'数据类型：', type(data_train))\n",
    "print(u'训练集包含的文本数目：', len(data_train.data))\n",
    "print(u'测试集包含的文本数目：', len(data_test.data))\n",
    "print(u'训练集和测试集使用的%d个类别的名称：' % len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = data_train.target_names  # 类别名字\n",
    "print(categories)\n",
    "\n",
    "y_train = data_train.target\n",
    "y_test = data_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u' -- 前10个文本 -- ')\n",
    "for i in np.arange(10):\n",
    "    print(u'文本%d(属于类别 - %s)：' % (i+1, categories[y_train[i]]))\n",
    "    print(data_train.data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对划分好了的文本进行向量化，这里用的是TF-idf方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input='content', stop_words='english', max_df=0.5, sublinear_tf=True)  # 使用TF-idf对文本向量化\n",
    "\n",
    "x_train = vectorizer.fit_transform(data_train.data)  # x_train是稀疏的，scipy.sparse.csr.csr_matrix\n",
    "x_test = vectorizer.transform(data_test.data)\n",
    "\n",
    "print(u'训练集样本个数：%d，特征个数：%d' % x_train.shape)\n",
    "print( u'停止词:\\n',)\n",
    "print(vectorizer.get_stop_words())\n",
    "\n",
    "feature_names = np.asarray(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.目标实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 罗列出所用的分类器\n",
    "clfs = (MultinomialNB(),                # 0.87(0.017), 0.002, 90.39%\n",
    "        BernoulliNB(),                  # 1.592(0.032), 0.010, 88.54%\n",
    "        KNeighborsClassifier(),         # 19.737(0.282), 0.208, 86.03%\n",
    "        RidgeClassifier(),              # 25.6(0.512), 0.003, 89.73%\n",
    "        RandomForestClassifier(n_estimators=200),   # 59.319(1.977), 0.248, 77.01%\n",
    "        SVC()                           # 236.59(5.258), 1.574, 90.10%\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用之前写好的函数进行机器学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "# 遍历所用之前定义的分类器，记下结果\n",
    "for clf in clfs:\n",
    "    a = test_clf(clf)\n",
    "    result.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.array(result)\n",
    "time_train, time_test, err, names = result.T\n",
    "x = np.arange(len(time_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['font.sans-serif'] = [u'simHei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "plt.figure(figsize=(10, 7), facecolor='w')\n",
    "ax = plt.axes()\n",
    "b1 = ax.bar(x, err, width=0.25, color='#77E0A0')\n",
    "\n",
    "# 打印的是两个y轴的坐标轴\n",
    "ax_t = ax.twinx()\n",
    "b2 = ax_t.bar(x+0.25, time_train, width=0.25, color='#FFA0A0')\n",
    "b3 = ax_t.bar(x+0.5, time_test, width=0.25, color='#FF8080')\n",
    "plt.xticks(x+0.5, names, fontsize=10)\n",
    "leg = plt.legend([b1[0], b2[0], b3[0]], (u'错误率', u'训练时间', u'测试时间'), loc='upper left', shadow=True)\n",
    "\n",
    "# for lt in leg.get_texts():\n",
    "#     lt.set_fontsize(14)\n",
    "\n",
    "plt.title(u'新闻组文本数据不同分类器间的比较', fontsize=18)\n",
    "plt.xlabel(u'分类器名称')\n",
    "plt.grid(True)\n",
    "plt.tight_layout(2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
