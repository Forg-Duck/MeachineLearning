{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "题目：\n",
    "\n",
    "为22.news.dat这篇文档中的长篇文章进行主题训练，使用LDA模型，并计算各部分用时(基本方法和01一样，就是文档大了许多)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import corpora, models, similarities\n",
    "from pprint import pprint\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个停止词函数\n",
    "def load_stopword():\n",
    "    f_stop = open('22.stopword.txt')\n",
    "    sw = [line.strip() for line in f_stop]\n",
    "    f_stop.close()\n",
    "    return sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入文本，并进行停止词的去除工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始读入语料数据 -- \n",
      "读入语料数据完成，用时11.921秒\n",
      "文本数目：2043个\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "\n",
    "stop_words = load_stopword()  # 初始化停止词列表\n",
    "\n",
    "# 读入文本\n",
    "print('开始读入语料数据 -- ')\n",
    "f = open('22.news.dat','rb')\n",
    "texts = [[word for word in line.strip().lower().split() if word not in stop_words] for line in f] # 去除停止词\n",
    "# texts = [line.strip().split() for line in f]\n",
    "print('读入语料数据完成，用时%.3f秒' % (time.time() - t_start))\n",
    "\n",
    "f.close()\n",
    "M = len(texts)\n",
    "print('文本数目：%d个' % M)\n",
    "# pprint(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行 TF-IDF 文档向量化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在计算文档TF-IDF --\n",
      "建立文档TF-IDF完成，用时0.227秒\n"
     ]
    }
   ],
   "source": [
    "# 建立字典用于TF-IDF\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "V = len(dictionary)\n",
    "\n",
    "# 计算文本向量\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "print('正在计算文档TF-IDF --')\n",
    "t_start = time.time()\n",
    "corpus_tfidf = models.TfidfModel(corpus)[corpus]\n",
    "print('建立文档TF-IDF完成，用时%.3f秒' % (time.time() - t_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.进行模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA模型完成，训练时间为\t10.797秒\n"
     ]
    }
   ],
   "source": [
    "# 进行LDA模型拟合推断\n",
    "num_topics = 30  # 主题设为30个\n",
    "t_start = time.time()\n",
    "\n",
    "# 进行模型训练 将之前处理好的 corpus_tfidf 文件喂给模型，主题30个，超参数为0.01\n",
    "lda = models.LdaModel(corpus_tfidf, num_topics=num_topics, id2word=dictionary,\n",
    "                        alpha=0.01, eta=0.01, minimum_probability=0.001,\n",
    "                        update_every = 1, chunksize = 100, passes = 1)\n",
    "print('LDA模型完成，训练时间为\\t%.3f秒' % (time.time() - t_start))\n",
    "\n",
    "#  所有文档的主题\n",
    "# doc_topic = [a for a in lda[corpus_tfidf]]\n",
    "# print 'Document-Topic:\\n'\n",
    "# pprint(doc_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.观察训练完成后的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个文档的主题分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机10个文档的主题分布：\n",
      "第550个文档的前10个主题：\n",
      "[0.38123742 0.28194046 0.13068601 0.09316909 0.00434489 0.00434489\n",
      " 0.00434489 0.00434489 0.00434489 0.00434489]\n",
      "第1729个文档的前10个主题：\n",
      "[0.53179723 0.2066005  0.08706684 0.05550412 0.04830942 0.00282888\n",
      " 0.00282888 0.00282888 0.00282888 0.00282888]\n",
      "第419个文档的前10个主题：\n",
      "[0.53000224 0.1510151  0.13183205 0.08676627 0.0486549  0.00206918\n",
      " 0.00206918 0.00206918 0.00206918 0.00206918]\n",
      "第714个文档的前10个主题：\n",
      "[0.44119251 0.25713351 0.09403287 0.07982886 0.06638387 0.00245713\n",
      " 0.00245713 0.00245713 0.00245713 0.00245713]\n",
      "第1818个文档的前10个主题：\n",
      "[0.42926741 0.27886125 0.16360036 0.05230379 0.00292181 0.00292181\n",
      " 0.00292181 0.00292181 0.00292181 0.00292181]\n",
      "第260个文档的前10个主题：\n",
      "[0.59973681 0.21033934 0.006783   0.00678299 0.00678299 0.00678299\n",
      " 0.00678299 0.00678299 0.00678299 0.00678299]\n",
      "第1786个文档的前10个主题：\n",
      "[0.50740391 0.16529153 0.13587341 0.07273821 0.00456512 0.00456512\n",
      " 0.00456512 0.00456512 0.00456512 0.00456512]\n",
      "第927个文档的前10个主题：\n",
      "[0.64895588 0.13678253 0.09498405 0.00441768 0.00441768 0.00441768\n",
      " 0.00441768 0.00441768 0.00441768 0.00441768]\n",
      "第935个文档的前10个主题：\n",
      "[0.55742824 0.18151812 0.10485793 0.09732825 0.00226413 0.00226413\n",
      " 0.00226413 0.00226413 0.00226413 0.00226413]\n",
      "第1118个文档的前10个主题：\n",
      "[0.63156343 0.15247577 0.10135572 0.04533423 0.04070271 0.00114273\n",
      " 0.00114273 0.00114273 0.00114273 0.00114273]\n"
     ]
    }
   ],
   "source": [
    "# 随机打印某10个文档的主题\n",
    "num_show_topic = 10  # 每个文档显示前几个主题\n",
    "print('随机10个文档的主题分布：')\n",
    "doc_topics = lda.get_document_topics(corpus_tfidf)  # 所有文档的主题分布\n",
    "idx = np.arange(M)\n",
    "np.random.shuffle(idx)  # 将文档打乱\n",
    "idx = idx[:10]\n",
    "\n",
    "for i in idx:  # 遍历选出的10个文档\n",
    "    topic = np.array(doc_topics[i])  # 用numpy的形式，方便操作\n",
    "    topic_distribute = np.array(topic[:, 1])\n",
    "    # print topic_distribute\n",
    "    topic_idx = topic_distribute.argsort()[:-num_show_topic-1:-1]\n",
    "    print('第%d个文档的前%d个主题：' % (i, num_show_topic)), topic_idx\n",
    "    print(topic_distribute[topic_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个主题的词分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题0:\n",
      "公司\n",
      "女士\n",
      "旅游\n",
      "创新\n",
      "村\n",
      "合作\n",
      "规划\n",
      "主题1:\n",
      "中方\n",
      "答\n",
      "航班\n",
      "美方\n",
      "联合国\n",
      "有何\n",
      "谈判\n",
      "主题2:\n",
      "韩国\n",
      "广岛\n",
      "奥巴马\n",
      "巴西\n",
      "英国\n",
      "原子弹\n",
      "加拿大\n",
      "主题3:\n",
      "算你狠\n",
      "白鹭洲\n",
      "翻篇\n",
      "缥缈\n",
      "练功\n",
      "约法三章\n",
      "老家伙\n",
      "主题4:\n",
      "仲裁\n",
      "领土\n",
      "案\n",
      "司法\n",
      "轿车\n",
      "绑架\n",
      "法官\n",
      "主题5:\n",
      "德国\n",
      "郑捷\n",
      "欧洲\n",
      "捷运\n",
      "死刑\n",
      "台北\n",
      "村民\n",
      "主题6:\n",
      "李某\n",
      "陈某\n",
      "考生\n",
      "招生\n",
      "宋某\n",
      "高考\n",
      "同比\n",
      "主题7:\n",
      "督察\n",
      "环保\n",
      "督察组\n",
      "列车\n",
      "河北\n",
      "铁路\n",
      "习近平\n",
      "主题8:\n",
      "算你狠\n",
      "白鹭洲\n",
      "翻篇\n",
      "缥缈\n",
      "练功\n",
      "约法三章\n",
      "老家伙\n",
      "主题9:\n",
      "司机\n",
      "检察院\n",
      "公交车\n",
      "公交\n",
      "车厢\n",
      "乘客\n",
      "车辆\n",
      "主题10:\n",
      "雷洋\n",
      "炸弹\n",
      "人大\n",
      "is\n",
      "爆炸\n",
      "伊拉克\n",
      "汽车\n",
      "主题11:\n",
      "日本\n",
      "南海\n",
      "菲律宾\n",
      "解决\n",
      "叙利亚\n",
      "共和党\n",
      "女生\n",
      "主题12:\n",
      "滨海\n",
      "女子\n",
      "行凶\n",
      "捅\n",
      "保安\n",
      "小球\n",
      "客服\n",
      "主题13:\n",
      "朝鲜\n",
      "金正恩\n",
      "地震\n",
      "朝鲜劳动党\n",
      "度\n",
      "东经\n",
      "震源\n",
      "主题14:\n",
      "签证\n",
      "用户\n",
      "高速\n",
      "法国\n",
      "百度\n",
      "欧元\n",
      "公司\n",
      "主题15:\n",
      "幼儿园\n",
      "仪征市\n",
      "园区\n",
      "学生家长\n",
      "搬迁\n",
      "实验\n",
      "新\n",
      "主题16:\n",
      "灾害\n",
      "福建\n",
      "搜救\n",
      "救援\n",
      "泰宁县\n",
      "泰宁\n",
      "泥石流\n",
      "主题17:\n",
      "特朗普\n",
      "总统\n",
      "民警\n",
      "医生\n",
      "着\n",
      "执法\n",
      "这些\n",
      "主题18:\n",
      "天气\n",
      "大风\n",
      "受灾\n",
      "广西\n",
      "紧急\n",
      "遇难\n",
      "住宅\n",
      "主题19:\n",
      "她\n",
      "女子\n",
      "视频\n",
      "日本\n",
      "治疗\n",
      "当地\n",
      "越南\n",
      "主题20:\n",
      "驾驶\n",
      "交警\n",
      "某某\n",
      "摩托车\n",
      "房主\n",
      "行驶\n",
      "违建\n",
      "主题21:\n",
      "先生\n",
      "保健品\n",
      "女性\n",
      "自由\n",
      "男性\n",
      "旅行社\n",
      "旅客\n",
      "主题22:\n",
      "蔡\n",
      "大陆\n",
      "英文\n",
      "台湾\n",
      "两岸\n",
      "马英九\n",
      "民进党\n",
      "主题23:\n",
      "任\n",
      "党委\n",
      "纪律\n",
      "巡视\n",
      "纪委\n",
      "市委\n",
      "纪委书记\n",
      "主题24:\n",
      "医院\n",
      "中国\n",
      "警方\n",
      "我\n",
      "美国\n",
      "他\n",
      "了\n",
      "主题25:\n",
      "阅读\n",
      "读书\n",
      "俄罗斯\n",
      "书籍\n",
      "电子\n",
      "意思\n",
      "普京\n",
      "主题26:\n",
      "被告人\n",
      "同志\n",
      "人民法院\n",
      "法庭\n",
      "中级\n",
      "行贿\n",
      "指控\n",
      "主题27:\n",
      "印尼\n",
      "充气\n",
      "帕廷\n",
      "天使\n",
      "娃娃\n",
      "神灵\n",
      "捡\n",
      "主题28:\n",
      "算你狠\n",
      "白鹭洲\n",
      "翻篇\n",
      "缥缈\n",
      "练功\n",
      "约法三章\n",
      "老家伙\n",
      "主题29:\n",
      "细胞\n",
      "临床\n",
      "有偿\n",
      "二院\n",
      "生物\n",
      "疗法\n",
      "患者\n"
     ]
    }
   ],
   "source": [
    "# 每个主题的词分布（共30个主题）\n",
    "num_show_term = 7   # 每个主题显示几个词\n",
    "\n",
    "for topic_id in range(num_topics):\n",
    "    print('主题%d:' % topic_id)\n",
    "    term_distribute_all = lda.get_topic_terms(topicid=topic_id)\n",
    "    term_distribute = term_distribute_all[:num_show_term]\n",
    "    term_distribute = np.array(term_distribute)\n",
    "    term_id = term_distribute[:, 0].astype(np.int)\n",
    "    for t in term_id:\n",
    "        print(dictionary.id2token[t])\n",
    "        # print '\\n概率：\\t', term_distribute[:, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
